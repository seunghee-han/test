# !! ì‚¬ìš©ì‹œ openai key, db ê³„ì • ë³€ê²½ í•„ìš”!!

import os
import json
from typing import List
from operator import itemgetter

import streamlit as st
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_community.vectorstores import FAISS
from langchain.retrievers import EnsembleRetriever
from langchain_postgres import PGVector
from langchain_core.messages import HumanMessage
import re

def render_answer(md: str) -> str:
    """
    - '<br>' í‘œê¸°ë¥¼ ì‹¤ì œ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³´ì´ê²Œ ì²˜ë¦¬
    - í‘œ ì…€ì—ì„œë„ ì¤„ë°”ê¿ˆì´ ë³´ì´ë„ë¡ HTML <br/> ë¡œ ì¹˜í™˜
    - ë¶ˆí•„ìš”í•œ ê³¼ë„í•œ ë¹ˆì¤„ ì •ë¦¬
    """
    # 1) ë‹¤ì–‘í•œ í˜•íƒœì˜ <br> íƒœê·¸ë¥¼ í†µì¼
    s = re.sub(r'<\s*br\s*/?\s*>', '<br/>', md, flags=re.I)

    # 2) ì—°ì† 3ì¤„ ì´ìƒ ë¹ˆì¤„ -> 2ì¤„ë¡œ ì¶•ì†Œ(ê³¼í•œ ê³µë°± ë°©ì§€)
    s = re.sub(r'\n{3,}', '\n\n', s)

    return s

# =====================
# ğŸ¨ Custom CSS for a beautiful UI
# =====================
st.markdown(
    """
<style>
/* =====================
   ì „ì²´ í˜ì´ì§€
===================== */
body {
    font-family: 'Inter', 'Segoe UI', sans-serif;
    background-color: #f7f8fa;
    color: #111827;
}

/* =====================
   ë©”ì¸ ì»¨í…Œì´ë„ˆ
===================== */
.main .block-container {
    padding: 2rem 2.5rem;
    max-width: 900px;
    margin: auto;
}

/* =====================
   ì‚¬ì´ë“œë°”
===================== */
[data-testid="stSidebar"] {
    background-color: #ffffff;
    border-right: 1px solid #e5e7eb;
    padding: 2rem 1rem;
    min-width: 220px;
}

[data-testid="stSidebar"] h2,
[data-testid="stSidebar"] h3 {
    color: #111827;
    font-weight: 600;
}

/* ì‚¬ì´ë“œë°” ë²„íŠ¼ */
[data-testid="stSidebar"] .stButton > button {
    width: 100%;
    text-align: left;
    padding: 12px 16px;
    border-radius: 10px;
    border: none;
    font-size: 0.95rem;
    color: #111827;
    background-color: #f3f4f6;
    margin-bottom: 0.5rem;
    font-weight: 500;
    transition: all 0.2s ease;
    box-shadow: 0 1px 3px rgba(0,0,0,0.05);
}
[data-testid="stSidebar"] .stButton > button:hover {
    background-color: #e5e7eb;
}
[data-testid="stSidebar"] .stButton[data-selected="true"] > button {
    background-color: #2563eb;
    color: #ffffff;
    font-weight: 600;
}

/* ìƒˆ ì±„íŒ… ë²„íŠ¼ ê°•ì¡° */
.stButton:nth-child(1) button {
    background-color: #2563eb;
    color: #ffffff !important;
    font-weight: 600;
}
.stButton:nth-child(1) button:hover {
    background-color: #1e40af !important;
}

/* =====================
   ì‚¬ì´ë“œë°” ë‹«í˜”ì„ ë•Œ ì „ì²´ í™•ì¥
===================== */
/* 1. ìµœìƒìœ„ ì»¨í…Œì´ë„ˆì—ì„œ ë‹«íŒ ì‚¬ì´ë“œë°” ìˆ¨ê¸°ê¸° */
[data-testid="stAppViewContainer"] > [data-testid="stSidebar"][aria-expanded="false"] {
    display: none !important;
    width: 0 !important;
    padding: 0 !important;
    margin: 0 !important;
}

/* 2. ì‚¬ì´ë“œë°” ë‹¤ìŒ .main ì˜ì—­ ë„ˆë¹„ 100% */
[data-testid="stAppViewContainer"] > [data-testid="stSidebar"][aria-expanded="false"] + .main {
    width: 100% !important;
    margin: 0 !important;
}

/* 3. .block-container ë‚´ë¶€ ì½˜í…ì¸  ê°€ë“ ì±„ìš°ê¸° */
[data-testid="stAppViewContainer"] > [data-testid="stSidebar"][aria-expanded="false"] + .main .block-container {
    max-width: none !important;
    width: calc(100% - 4rem) !important;
    margin: auto !important;
}

/* =====================
   ì±„íŒ… ë©”ì‹œì§€ ì¹´ë“œ
===================== */
[data-testid="stChatMessage"] {
    border-radius: 16px;
    padding: 1rem 1.2rem;
    margin-bottom: 1rem;
    box-shadow: 0 2px 8px rgba(0,0,0,0.06);
    max-width: 100%;
    line-height: 1;
    font-size: 0.95rem;
    word-wrap: break-word;
    white-space: pre-wrap;
}

/* ì‚¬ìš©ì ë©”ì‹œì§€ (ì˜¤ë¥¸ìª½) */
.st-emotion-cache-1c0t8v9 {
    background-color: #2563eb !important;
    color: #ffffff !important;
    border-top-left-radius: 0 !important;
    margin-left: auto;
}

/* ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ (ì™¼ìª½) */
.st-emotion-cache-x7gh8c {
    background-color: #f3f4f6 !important;
    color: #111827 !important;
    border-top-right-radius: 0 !important;
    margin-right: auto;
}

/* =====================
   ì…ë ¥ì°½
===================== */
[data-testid="stTextInput"] > div > div > input {
    border-radius: 999px;
    border: 1px solid #d1d5db;
    padding: 14px 20px;
    font-size: 0.95rem;
    transition: all 0.2s ease;
    background-color: #ffffff;
    width: calc(100% - 40px);
}
[data-testid="stTextInput"] > div > div > input:focus {
    border-color: #2563eb;
    box-shadow: 0 0 0 4px rgba(37, 99, 235, 0.2);
}

/* =====================
   ì œëª©
===================== */
h1 {
    color: #111827;
    font-weight: 700;
    font-size: 2rem;
    margin-bottom: 1rem;
}

/* =====================
   íŒŒì¼ ì—…ë¡œë“œ
===================== */
.stFileUpload {
    border-radius: 14px;
    border: 1px dashed #d1d5db;
    padding: 1rem;
    background-color: #ffffff;
}

/* =====================
   êµ¬ë¶„ì„ 
===================== */
hr {
    border: none;
    border-top: 1px solid #e5e7eb;
    margin: 1.5rem 0;
}

/* =====================
   ëª¨ë°”ì¼/ë°˜ì‘í˜•
===================== */
@media (max-width: 768px) {
    .main .block-container {
        padding: 1rem 1rem;
    }
    [data-testid="stSidebar"] {
        min-width: 180px;
        padding: 1rem 0.5rem;
    }
    [data-testid="stChatMessage"] {
        max-width: 90%;
    }
}
</style>
""",
    unsafe_allow_html=True,
)



# =====================
# âš™ï¸ App Config
# =====================
st.set_page_config(page_title="ìš´ì „ì ë³´í—˜ ì±—ë´‡ ì„œë¹„ìŠ¤", layout="wide")
st.title("ìš´ì „ì ë³´í—˜ ì±—ë´‡ ì„œë¹„ìŠ¤")

# --- Environment ---
# change
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    st.warning(
        "âš ï¸ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env ë˜ëŠ” ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”."
    )

connection_string = os.getenv(
    "PG_CONN",
    # change
    "postgresql+psycopg2://",
)


# =====================
# ğŸ”§ Engine (ì»¤ë„¥ì…˜ í’€ ìºì‹œë¡œ Too many clients ë°©ì§€)
# =====================
@st.cache_resource(show_spinner=False)
def get_engine(conn_str: str) -> Engine:
    """ì»¤ë„¥ì…˜ í’€ì´ ì ìš©ëœ ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    return create_engine(
        conn_str,
        pool_size=5,
        max_overflow=0,
        pool_pre_ping=True,
        pool_recycle=1800,
    )


# =====================
# ì„¸ì…˜ ê´€ë¦¬
# =====================
def generate_title(message):
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ì˜ ì œëª©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    prompt = f"ì´ ë©”ì‹œì§€ë¥¼ 5~10ì ì´ë‚´ë¡œ ê°„ë‹¨íˆ ì œëª©í™” í•´ì¤˜:\n\n{message}"
    response = ChatOpenAI(model="gpt-4.1-2025-04-14", api_key=OPENAI_API_KEY).invoke(
        [HumanMessage(content=prompt)]
    )
    return response.content.strip()


# ì´ˆê¸° ì„¸ì…˜ ìƒíƒœ ì„¤ì •
if "conversations" not in st.session_state:
    st.session_state["conversations"] = [
        {"title": "ëŒ€í™” ë‚´ìš©ì´ ê¸°ë¡ë©ë‹ˆë‹¤", "messages": []}
    ]

if "current_chat" not in st.session_state:
    st.session_state["current_chat"] = 0

if "dev_mode" not in st.session_state:
    st.session_state["dev_mode"] = False


# =====================
# ğŸ’¬ Sidebar
# =====================
with st.sidebar:
    st.subheader("âœ¨ ì‹œì‘í•˜ê¸°")

    # 'ìƒˆ ì±„íŒ…' ë²„íŠ¼
    if st.button("â• ìƒˆ ì±„íŒ…"):
        st.session_state["conversations"].append(
            {"title": "ëŒ€í™” ë‚´ìš©ì´ ê¸°ë¡ë©ë‹ˆë‹¤", "messages": []}
        )
        st.session_state["current_chat"] = len(st.session_state["conversations"]) - 1
        st.rerun()

    st.markdown("---")
    st.subheader("ğŸ’¬ ì±„íŒ… ê¸°ë¡")

    # ì±„íŒ… ê¸°ë¡ ë²„íŠ¼
    for i, session in enumerate(st.session_state["conversations"]):
        button_label = session["title"]
        is_selected = i == st.session_state["current_chat"]

        # Streamlit ë²„íŠ¼ì— data-selected ì†ì„±ì„ ì¶”ê°€í•˜ì—¬ CSSë¡œ ìŠ¤íƒ€ì¼ë§
        button_container = st.container()
        if button_container.button(button_label, key=f"chat_{i}"):
            st.session_state["current_chat"] = i
            st.rerun()

        # ì„ íƒëœ ë²„íŠ¼ì— CSS í´ë˜ìŠ¤ë¥¼ ì¶”ê°€í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (ë‹¤ì†Œ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ)
        if is_selected:
            st.markdown(
                f"""
                <script>
                    const button = window.parent.document.querySelector('[data-testid="stSidebar"] button[key="chat_{i}"]');
                    if (button) {{
                        button.parentElement.setAttribute('data-selected', 'true');
                    }}
                </script>
            """,
                unsafe_allow_html=True,
            )

    st.markdown("---")
    st.subheader("ğŸ“¤ ê°œì¸ íŒŒì¼ ì—…ë¡œë“œ")
    uploads = st.file_uploader(
        "PDF ë˜ëŠ” TXT ì—…ë¡œë“œ", type=["pdf", "txt"], accept_multiple_files=True
    )

    st.markdown("---")
    st.session_state["dev_mode"] = st.checkbox(
        "ê°œë°œì ëª¨ë“œ", value=st.session_state["dev_mode"]
    )

# =====================
# ğŸ§  LLM/Embedding
# =====================
llm = ChatOpenAI(model="gpt-4.1-2025-04-14", api_key=OPENAI_API_KEY)
base_embed = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
cache_dir = "./mycache/embedding"
os.makedirs(cache_dir, exist_ok=True)
embed_store = LocalFileStore(cache_dir)
cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings=base_embed,
    document_embedding_cache=embed_store,
)


# =====================
# ğŸ—‚ï¸ Helpers (DB)
# =====================
@st.cache_data(show_spinner=False)
def list_collections(conn_str: str) -> List[str]:
    """ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ì»¬ë ‰ì…˜ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        engine = get_engine(conn_str)
        with engine.connect() as conn:
            rows = conn.execute(
                text("SELECT name FROM public.langchain_pg_collection ORDER BY name")
            )
            return [(r[0] or "").strip() for r in rows]
    except Exception as e:
        st.error(f"ì»¬ë ‰ì…˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []


@st.cache_resource(show_spinner=False)
def build_pg_retrievers(conn_str: str, collections: List[str]):
    """PGVectorì—ì„œ ê²€ìƒ‰ê¸°ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤."""
    retrievers = []
    skipped = []
    engine = get_engine(conn_str)
    with engine.connect() as conn:
        existing = {
            (r[0] or "").strip(): r[1]
            for r in conn.execute(
                text("SELECT name, uuid FROM public.langchain_pg_collection")
            )
        }

    for raw_name in collections:
        cname = (raw_name or "").strip()
        if not cname or cname not in existing:
            skipped.append(raw_name)
            continue
        vs = PGVector(
            embeddings=base_embed,
            collection_name=cname,
            connection=conn_str,
            use_jsonb=True,
        )
        try:
            _ = vs.similarity_search("ping", k=1)
        except ValueError as e:
            if "Collection not found" in str(e):
                skipped.append(cname)
                continue
            else:
                raise
        retrievers.append(
            vs.as_retriever(
                search_type="mmr",
                search_kwargs={"k": 4, "fetch_k": 20, "lambda_mult": 0.8},
            )
        )
    if skipped:
        st.warning(
            f"ë‹¤ìŒ ì»¬ë ‰ì…˜ì€ ê±´ë„ˆëœ€(ì¡´ì¬/ì ‘ê·¼ ë¬¸ì œ): {', '.join([s for s in skipped if s])}"
        )
    return retrievers


# =====================
# ğŸ“„ Helpers (Uploads)
# =====================
@st.cache_resource(show_spinner="ì—…ë¡œë“œ íŒŒì¼ ì„ë² ë”© ì¤‘â€¦")
def build_upload_retriever(files, chunk_size=1000, chunk_overlap=80):
    """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ ê²€ìƒ‰ê¸°ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤."""
    from langchain_core.documents import Document

    save_dir = "./mycache/files"
    os.makedirs(save_dir, exist_ok=True)
    docs = []
    for f in files:
        name = f.name
        ext = name.split(".")[-1].lower()
        save_path = os.path.join(save_dir, name)
        bin_bytes = f.read()
        with open(save_path, "wb") as fp:
            fp.write(bin_bytes)
        if ext == "pdf":
            loader = PDFPlumberLoader(save_path)
            file_docs = loader.load()
        elif ext == "txt":
            text = bin_bytes.decode("utf-8")
            file_docs = [
                Document(page_content=text, metadata={"source": save_path, "page": 1})
            ]
        else:
            continue
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap
        )
        docs.extend(splitter.split_documents(file_docs))
    if not docs:
        return None
    vs = FAISS.from_documents(docs, cached_embedder)
    return vs.as_retriever()


# =====================
# ğŸ” Retriever Assembly (DB + Uploads)
# =====================
all_retrievers = []
all_collections = list_collections(connection_string)
if all_collections:
    db_retrievers = build_pg_retrievers(connection_string, all_collections)
    all_retrievers.extend(db_retrievers)
else:
    st.info("DBì— ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ì—…ë¡œë“œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

upload_retriever = None
if "uploads" in locals() and uploads:
    upload_retriever = build_upload_retriever(uploads)
    if upload_retriever is not None:
        all_retrievers.append(upload_retriever)

final_retriever = None
if len(all_retrievers) == 1:
    final_retriever = all_retrievers[0]
elif len(all_retrievers) > 1:
    final_retriever = EnsembleRetriever(retrievers=all_retrievers)

# =====================
# ğŸ§© RAG Chain
# =====================
prompt = ChatPromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì—¬ëŸ¬ ë³´í—˜ì‚¬ì˜ ìƒí’ˆì„ ë¹„êµ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ì‰½ê³  ì¹œì ˆí•˜ê²Œ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” ë³´í—˜ ì „ë¬¸ê°€ ì±—ë´‡ì…ë‹ˆë‹¤.

ë‹¤ìŒ ì§€ì‹œì‚¬í•­ì„ ì² ì €íˆ ë”°ë¼ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ë‹µë³€ì˜ ì„œì‹:
- ë§ˆí¬ë‹¤ìš´(Markdown)ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ êµ¬ì¡°í™”í•˜ì„¸ìš”.
- ì¤„ë°”ê¿ˆì€ ìµœëŒ€ ë‘ ì¹¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- ì£¼ìš” ì •ë³´ëŠ” **ë³¼ë“œì²´**ë¡œ ê°•ì¡°í•˜ì„¸ìš”.

- ëª¨ë“  ì„¹ì…˜ì€ ë°˜ë“œì‹œ ì•„ë˜ ê³„ì¸µì„ ë”°ë¦…ë‹ˆë‹¤.
  1) ì œëª©: `##`  (í•œ ë‹µë³€ì— 2~4ê°œ ì´í•˜ ê¶Œì¥)
  2) ì†Œì œëª©: `###`  (ìƒí’ˆëª…/í•­ëª©ëª…/ì†Œë‹¨ë½ ì œëª©ì€ ë°˜ë“œì‹œ ì†Œì œëª©ìœ¼ë¡œ, ë¶ˆë¦¿ ê¸ˆì§€)
  3) ì„¸ë¶€ í¬ì¸íŠ¸: `- ` ë¶ˆë¦¿ ëª©ë¡ (ê° ì†Œì œëª© ì•„ë˜ ì„¤ëª…ì€ ëª¨ë‘ ë¶ˆë¦¿ìœ¼ë¡œ ì •ë¦¬)
- ë¶ˆë¦¿ì€ **â€œ- â€ + ê³µë°± + í…ìŠ¤íŠ¸** í˜•ì‹ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- í—¤ë”© ì•ë’¤ ë¹ˆ ì¤„ì€ **ë”± 1ì¤„**ë§Œ, ë¶ˆë¦¿ ì‚¬ì´ì—ëŠ” **ë¹ˆ ì¤„ ê¸ˆì§€**.

- ì—¬ëŸ¬ íšŒì‚¬ë¥¼ ë¹„êµí•˜ëŠ” ê²½ìš°, ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í‘œë¥¼ í™œìš©í•˜ì—¬ ì„¤ëª…ê³¼ í•¨ê»˜ í•­ëª©ë³„ë¡œ ì •ë¦¬í•˜ì„¸ìš”.
- í‘œë¥¼ ë§Œë“¤ ë•ŒëŠ” í•„ìš”ì‹œ **'ì¶”ì²œë„' ì—´**ì„ í¬í•¨í•˜ì„¸ìš”.
- ì¶”ì²œë„ ì—´ì—ë§Œ â­• / â–² / â—‹(ë³´í†µ) / Ã— / - ë‹¤ì„¯ ê°€ì§€ ê¸°í˜¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- ì¶”ì²œë„ ì—´ ì‚¬ìš©ì‹œ, í‘œ ë°”ë¡œ ì•„ë˜ì¤„ì— ë°˜ë“œì‹œ ë²”ë¡€ë¥¼ ì¶”ê°€í•˜ì„¸ìš”: â­• ê°•ë ¥ ì¶”ì²œ / â–² ì¡°ê±´ë¶€ / â—‹ ë³´í†µ / Ã— ì—†ìŒ / - ë¯¸í™•ì¸


ë‹µë³€ì˜ ë‚´ìš©:
- ì „ë¬¸ ìš©ì–´ëŠ” í”¼í•˜ê³ , ëˆ„êµ¬ë‚˜ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê³  ìì„¸í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
- ë‹µë³€ì€ ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‚´ìš©ë§Œì„ í¬í•¨í•˜ë©°, ë¶ˆí•„ìš”í•œ ì„œë¡ ì´ë‚˜ ê²°ë¡ ì€ ì œì™¸í•˜ì„¸ìš”.
- ì œì‹œëœ 'ì»¨í…ìŠ¤íŠ¸' ë‚´ì˜ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ë¡œ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
- ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì§ˆë¬¸í•  ì‹œ, ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì§€ë§ê³  ì§ˆë¬¸í•œ íšŒì‚¬ì˜ ì „í™”ë²ˆí˜¸ë‚˜ ë§í¬ë¥¼ ì¤˜ì„œ ì§ì ‘ ë‚´ìš©ì„ íƒìƒ‰í•˜ë„ë¡ ìœ ë„í•˜ì„¸ìš”.
- ë¹„êµí• ë•ŒëŠ” ì°¨ì´ì ì„ ëª…í™•í•˜ê²Œ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
- ì¶”ì²œ ì‹œì—ëŠ” ê·¸ ì´ìœ (ë³´ì¥ ë²”ìœ„, íŠ¹í™” ì˜µì…˜, ì§€ê¸‰ ì¡°ê±´ ë“±)ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
- ì‚¬ìš©ì ìƒí™©(ì‚¬ê³  ìœ í˜•/ê³¼ì‹¤/ì—°ë ¹ ë“±)ì„ ë°˜ì˜í•´ ì°¨ì´ì Â·ì¶”ì²œ ê·¼ê±°ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œ.

ì¶œì²˜ í‘œê¸°:
- ë‹µë³€ì— ì‚¬ìš©ëœ ëª¨ë“  ì •ë³´ëŠ” ë§ˆì§€ë§‰ì— ì¶œì²˜(íŒŒì¼ëª…/í˜ì´ì§€)ë¥¼ ëª…í™•íˆ ëª…ì‹œí•˜ì„¸ìš”.
- ì¶œì²˜ëŠ” í•­ìƒ ê´„í˜¸(()) ì•ˆì— (ì¶œì²˜: íŒŒì¼ëª…, í˜ì´ì§€ p.N) í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.

ë§ˆì§€ë§‰ì—ëŠ” í•­ìƒ ë” ë¬¼ì–´ë³¼ ë‚´ìš©ì´ ìˆëŠ”ì§€ ë¬¼ì–´ë³´ë©´ì„œ ëŒ€í™”ë¥¼ ë” ê¸¸ê²Œí•˜ë„ë¡ ìœ ë„í•´

[ëŒ€í™” ê¸°ë¡]
{history}

[ì§ˆë¬¸]
{question}

[ì»¨í…ìŠ¤íŠ¸]
{context}
"""
)


def format_docs(docs):
    lines = []
    for d in docs:
        src = d.metadata.get("source", "")
        page = d.metadata.get("page", "")
        import os as _os

        lines.append(
            f"{d.page_content}\n[ì¶œì²˜: {_os.path.basename(src)}, í˜ì´ì§€ p.{page}]"
        )
    return "---".join(lines)


def get_history_text(chat):
    """ëŒ€í™” ê¸°ë¡ì„ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    return "\n".join([f"{m['role']}: {m['content']}" for m in chat["messages"]])


# =====================
# ğŸ’¬ Chat UI
# =====================
current_chat = st.session_state["conversations"][st.session_state["current_chat"]]

# ê¸°ì¡´ ëŒ€í™” ë¨¼ì € ì¶œë ¥
for m in current_chat["messages"]:
    st.chat_message(m["role"]).write(m["content"])

# ì…ë ¥ì°½ì„ ë§¨ ì•„ë˜ì— ê³ ì •
user_q = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”â€¦")

if user_q:
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    current_chat["messages"].append({"role": "user", "content": user_q})
    if len(current_chat["messages"]) == 1:
        try:
            current_chat["title"] = generate_title(user_q)
        except Exception:
            current_chat["title"] = user_q[:20] + ("..." if len(user_q) > 20 else "")

    # ì‚¬ìš©ì ë©”ì‹œì§€ UIì— í‘œì‹œ
    st.chat_message("user").write(user_q)

    # RAG ì²´ì¸ í˜¸ì¶œ
    if final_retriever is None:
        st.error("í™œì„±í™”ëœ ë°ì´í„° ì†ŒìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (DBì— ì»¬ë ‰ì…˜ì´ ì—†ê³  ì—…ë¡œë“œë„ ì—†ìŒ)")
    else:
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                history_text = get_history_text(current_chat)
                # rag_chain = (
                #     RunnablePassthrough.assign(
                #         context=final_retriever | format_docs,
                #         history=lambda x: history_text,
                #     )
                #     | prompt
                #     | llm
                #     | StrOutputParser()
                # )
                # answer = rag_chain.invoke({"question": user_q})
                rag_chain = (
                    {
                        "question": itemgetter("question"),
                        "history": lambda _: history_text,
                        "context": itemgetter("question") | final_retriever | format_docs,
                    }
                    | prompt
                    | llm
                    | StrOutputParser()
                )

                answer = rag_chain.invoke({"question": user_q})
                st.write(answer)


                # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
                current_chat["messages"].append(
                    {"role": "assistant", "content": answer}
                )

st.markdown("---")

# =====================
# ğŸ” Debug / Info (optional)
# =====================
if st.session_state["dev_mode"]:
    with st.expander("ë””ë²„ê·¸/ì •ë³´"):

        def _mask(cs: str | None) -> str:
            if not cs:
                return "(ì—†ìŒ)"
            return f"{cs[:8]}*** (ë§ˆìŠ¤í‚¹ë¨)"

        st.write(
            {
                "connection_string": _mask(connection_string),
                "collections": all_collections,
                "has_upload_retriever": upload_retriever is not None,
            }
        )
        st.info(
            "Tip: ì´ ì•±ì€ ì¡°íšŒìš©ì…ë‹ˆë‹¤. ìƒ‰ì¸ì€ ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ìˆ˜í–‰í•˜ê³ , pre_delete_collection=TrueëŠ” ê±°ê¸°ì„œë§Œ ì‚¬ìš©í•˜ì„¸ìš”."
        )
